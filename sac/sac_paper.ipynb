{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c6d61d-a712-494c-9b07-223d40016aa9",
   "metadata": {},
   "source": [
    "## 1. 最大熵强化学习\n",
    "- [Soft Actor-Critic:Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/pdf/1801.01290)\n",
    "  - 离线策略算法\n",
    "  - SAC：基于Soft Q-learning改进而来，使用显示的actor函数\n",
    "\n",
    "\n",
    "- [Soft Q-learning](https://arxiv.org/pdf/1702.08165)\n",
    "\n",
    "  - 不学习显示的策略函数，而是使用一个Q函数的玻尔兹曼分布\n",
    "  - learning expressive energy-based policies for continuous states and actions\n",
    "  - 增强探索能力\n",
    "\n",
    "对于一个随机变量X,它的概率密度函数为p，它的熵H定义为：\n",
    "$$\n",
    "H(X) = \\mathbb{E}_{x\\sim p}[-\\log{p(x)}]\n",
    "$$\n",
    "强化学习中可以使用$H(\\pi(*|s))$表示策略$\\pi$在状态$s$下的探索能力（随机程度）。\n",
    "\n",
    "\n",
    "#### 标准强化学习\n",
    "考虑以下infinite-horizon Markov Decision Process$(S, A, p_s, r)$，状态空间$S$和动作空间$A$是连续的:\n",
    "- 状态转移方程$p_s: S \\times S \\times A \\rightarrow [0, \\infty]$\n",
    "- 奖励函数$r: S\\times A\\rightarrow [r_{min}, r_{max}]$\n",
    "- $\\rho_{\\pi}(s_t)$和$\\rho_{\\pi}(s_t, a_t)$分别表示state and state-action marginals of the trajectory distribution induced by a policy $\\pi(a_t|s_t)$\n",
    "标准强化学习目标是学习以下最优策略：\n",
    "$$\n",
    "\\pi_{std}^{*} = \\arg\\max_{\\pi}\\sum_{t}\\mathbb{E}_{(s_t,a_t)\\sim \\rho_{\\pi}}[r(s_t,a_t)]\n",
    "$$\n",
    "\n",
    "#### 最大熵强化学习\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pi_{maxEnt}^{*} &= \\arg\\max_{\\pi}\\sum_{t}\\mathbb{E}_{(s_t,a_t)\\sim \\rho_{\\pi}}[r(s_t,a_t)+\\alpha H(\\pi(*|s_t))]\\\\\n",
    "&= \\arg\\max_{\\pi}\\sum_{t}\\mathbb{E}_{(s_t)\\sim \\rho_{\\pi}}[\\sum r(s_t,a_t)+\\alpha H(\\pi(*|s_t))]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "通过最大化每个状态$s_t$的熵从而增加每个状态$s_t$下的策略探索能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190a582-fd36-4ca7-b23d-ed73c64eb6b8",
   "metadata": {},
   "source": [
    "## 2. SAC：soft actro-critic\n",
    "\n",
    "#### 动作价值函数Q\n",
    "SAC算法包含两个动作价值函数$Q_1$、$Q_2$和一个策略函数$\\pi$：\n",
    "- 采用两个Q函数缓解Q值估计过高的问题：每次选择$min(Q_1(s,a),Q_2(s,a))$\n",
    "- 同时采用目标网络$Q_1^{target}$、$Q_2^{target}$\n",
    "\n",
    "每个Q网络的损失函数如下，而目标网络更新方式跟DDPG算法中目标网络一样：\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{\\omega} &=\\mathbb{E}_{(s_t,a_t,r_t,s_{t+1}\\sim R)}[(Q_{\\omega}(s_t,a_t)- (r_t+\\gamma(\\underset{j=1,2}{min}Q_{\\omega_j^{target}}(s_{t+1},a_{t+1}))-\\alpha\\log{\\pi(a_{t+s}|s_{t+1})})))^2] \\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### 策略函数$\\pi$\n",
    "SAC的策略目标可以理解为最大化状态价值函数:\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_t) &= \\mathbb{E}_{a_t\\sim\\pi}[\\underset{s_t}{\\sum}Q(s_t,a_t) + \\alpha H(\\pi(*|s_t))]\\\\\n",
    "&= \\color{red}\\mathbb{E}_{a_t\\sim\\pi}[\\underset{s_t}{\\sum}[Q(s_t,a_t) - \\alpha\\log(\\pi(a_t|s_t))]]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "因此策略网络的最小化目标损失函数为\n",
    "$$\n",
    "L_{\\pi}(\\theta)= \\mathbb{E}[\\alpha\\log(\\pi_{\\theta}(a_t|s_t))-Q_{\\omega}(s_t,a_t)]\n",
    "$$\n",
    "\n",
    "对于连续动作问题，SAC策略输出高斯分布的均值和方差\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde4235-ed6f-483a-b21b-c3e960950c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
