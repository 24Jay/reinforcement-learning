{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4618603-83c4-4b3f-a8ab-8df9c1e07960",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50da998b-ae98-4f99-af53-5e4edc5a280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeffa910-d4f0-4405-b6fe-193e85b9c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard import notebook\n",
    "\n",
    "# # 初始化 SummaryWriter，日志将保存在'runs'目录下\n",
    "# writer = SummaryWriter(f\"./logs/{datetime.datetime.now()}\")\n",
    "\n",
    "# # 启动 TensorBoard 并指定日志目录\n",
    "# notebook.start(\"--logdir runs\")\n",
    "\n",
    "# # 可选：创建一个链接直接跳转到 TensorBoard 界面\n",
    "# # notebook.display(height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7984d827-5006-4aab-b263-ad3657f90536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_state, n_action, hidden_size = 64):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(n_state, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_mean = torch.nn.Linear(hidden_size, n_action)\n",
    "        self.fc_std = torch.nn.Linear(hidden_size, n_action)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(F.relu(x))\n",
    "        mu = torch.tanh(self.fc_mean(F.relu(x))) * 1.0\n",
    "        std = F.softplus(self.fc_std(F.relu(x))) #+ 0e-3\n",
    "        # std = torch.ones_like(mu) * 1.0  # 固定标准差，增强探索\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_state, hidden_size=64):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(n_state, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(F.relu(x))\n",
    "        x = self.fc3(F.relu(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_advantage(gamma, lmbda, td_delta):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0.0\n",
    "    for delta in td_delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    return torch.tensor(advantage_list, dtype=torch.float)\n",
    "\n",
    "def shaped_reward(state, original_reward):\n",
    "    position, velocity = state\n",
    "    # 增加与位置相关的奖励（鼓励向右侧山顶移动）\n",
    "    position_reward = 10 * (position + 0.5)  # 谷底位置约为 -0.5，向右侧移动时奖励增加\n",
    "    # 增加与速度方向相关的奖励（鼓励沿目标方向加速）\n",
    "    velocity_reward = 5 * velocity if position > -0.5 else 0  # 右侧加速时奖励更高\n",
    "    return original_reward + position_reward + velocity_reward\n",
    "\n",
    "class PPOContinuous(nn.Module):\n",
    "\n",
    "    def __init__(self, n_state, n_action, n_hidden = 64, actor_lr=1e-4, critic_lr=1e-4, lmbda=0.1, epochs=10, eps=0.01, gamma=0.99, device=\"cpu\"):\n",
    "        super(PPOContinuous, self).__init__()\n",
    "        print(f\"{n_state=}, {n_action=}, {n_hidden=}\")\n",
    "\n",
    "        self.actor = Actor(n_state, n_action, hidden_size=n_hidden)\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "\n",
    "        self.critic = Critic(n_state, hidden_size=n_hidden)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr = critic_lr)\n",
    "\n",
    "        self.lmbda = lmbda\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state, eval = False):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        mu, std = self.actor(state)\n",
    "\n",
    "        action_dist = torch.distributions.Normal(mu, std)\n",
    "        action = action_dist.sample()\n",
    "\n",
    "        if not eval:\n",
    "            r = random.uniform(-1, 1) * 0.0\n",
    "        else:\n",
    "            r = 0.\n",
    "\n",
    "        # return [np.clip(action.item() + r, -1, 1)]\n",
    "        return [action.item()]\n",
    "        \n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device) \n",
    "\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        \n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta)\n",
    "\n",
    "        mu, std = self.actor(states)\n",
    "        action_dist = torch.distributions.Normal(mu.detach(), std.detach())\n",
    "        old_log_probs = action_dist.log_prob(actions)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            mu, std = self.actor(states)\n",
    "            # print(f\"====={mu.max()=}, {std.max()=}, {std.min()=}\")\n",
    "\n",
    "            action_dists = torch.distributions.Normal(mu, std)\n",
    "            log_probs = action_dists.log_prob(actions)\n",
    "\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            # ratio = torch.clamp(ratio, min=-1e6, max=1e6)\n",
    "            l1 = ratio * advantage\n",
    "            l2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage\n",
    "            l3 = - torch.min(l1, l2)\n",
    "\n",
    "            actor_loss = torch.mean(l3)\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            # print(f\"====={actor_loss=}, {critic_loss=}, {l1.min()=}, {ratio.min()=}, {ratio.max()=} \\n\")\n",
    "            \n",
    "\n",
    "            self.actor_opt.zero_grad()\n",
    "            self.critic_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_opt.step()\n",
    "            self.critic_opt.step()\n",
    "        # print(f\"====actor_loss: {actor_loss.detach().item()}, critic_loss: {critic_loss.detach().item()}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33071044-9c76-4f06-8f27-1e9bc44e5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space: [-0.34490016  0.01117477]\n",
      "action space: Box(-1.0, 1.0, (1,), float32)\n",
      "n_state=2, n_action=1, n_hidden=128\n"
     ]
    }
   ],
   "source": [
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 1000\n",
    "hidden = 128\n",
    "\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "epochs = 10\n",
    "eps = 0.4\n",
    "\n",
    "device = \"cpu\"\n",
    "env_name = \"MountainCarContinuous-v0\"\n",
    "# env_name = \"Pendulum-v1\"\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "torch.manual_seed(0)\n",
    "print(f\"state space:\" , env.observation_space.sample())\n",
    "print(f\"action space: {env.action_space}\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = PPOContinuous(n_state=state_dim, n_action=action_dim, n_hidden=hidden,actor_lr=actor_lr, critic_lr=critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e116f6-b5d9-411f-8a21-ce736125d39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8076975cb1b84b0dbe925717ebc02791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###0 : 0 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f7f4cdee25410ebea7ff8808973bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###1 : -0.048113669103057706 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e34097510740fdbe223a272bb03c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###2 : -0.3388021284478305 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f96f2532f9425c9abddd29c389d54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###3 : -0.016599048092557106 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c04f3f31da4b66b2b1682832762b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###4 : -0.049432106351014415 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eecc16a333473bba7f303ee796857f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "###5 : -0.011318589448255494 :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_on_policy_agent(env, agent, num_episodes):\n",
    "    return_list = []\n",
    "    episode_return = 0\n",
    "\n",
    "    for epoch in range(num_episodes//100):\n",
    "    \n",
    "        for i in tqdm(range(100), position=0, desc=f\"###{epoch} : {episode_return} \"):\n",
    "            episode_return = 0\n",
    "            transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
    "            state, _ = env.reset()\n",
    "            done , truncated = False, False\n",
    "            while not done and not truncated:\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                # reward = shaped_reward(state, reward)\n",
    "                \n",
    "    \n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "            return_list.append(episode_return)\n",
    "            agent.update(transition_dict)\n",
    "\n",
    "\n",
    "    return return_list\n",
    "        \n",
    "return_list = train_on_policy_agent(env, agent, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad1a54-cf7c-4062-b51c-8687380251e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(return_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd2750bc-4eb2-4a62-8c79-db79cbaf36e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting observation: [-0.48672822  0.        ]\n",
      "Episode finished! Total reward: 71.94054435719826\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "def test_agent(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "    state, info = env.reset()\n",
    "\n",
    "    print(f\"Starting observation: {state}\")\n",
    "\n",
    "    episode_over = False\n",
    "    total_reward = 0\n",
    "    action_list = []\n",
    "\n",
    "    while not episode_over:\n",
    "        action = agent.take_action(state, eval = True)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_over = terminated or truncated\n",
    "        # print(f\"{action=}\")\n",
    "        action_list.append(action)\n",
    "\n",
    "    print(f\"Episode finished! Total reward: {total_reward}\")\n",
    "    env.close()\n",
    "    return action_list\n",
    "    \n",
    "\n",
    "action_list = test_agent(agent, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a2a1a2-d131-4eb0-8539-debdb5f56c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHuVJREFUeJzt3X+QVtV9P/DPorBohA0k/GZBWixIEFAUBTsBExQoQ+GP2oyTCcQqUx3oSO1gu5lUp1pn/dYQJJEC1ihpEgajEZgm+INikUkXVH41gBNmSKy7Gn7EVlmgERx4vnPvzG5c3V19ED3u87xeM2d273nO3eeefZ7dfe+559xbUSgUCgEAkEinVE8MAJARRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEjq3OgATp8+Hb/5zW+iW7duUVFRkfpwAIAPIbuu6tGjR6N///7RqVOnjh1GsiBSXV2d+jAAgDPQ0NAQAwcO7NhhJBsRaepM9+7dUx8OAPAhNDY25oMJTX/HO3QYaTo1kwURYQQAOpYPmmJhAisAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYzQITTccutH2n/N//uHs3YsdBz/9V9zP7DNTStfivkb53+or/e1X/w6OpqfLf2vD/zevLFy7yd4RPB+wggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAAAdN4zcd999UVFREQsWLGi33eOPPx7Dhw+Prl27xiWXXBLr16//KE8LAJSQMw4jL730UqxYsSJGjRrVbru6urq44YYb4qabboqdO3fGrFmz8rJnz54zfWoAoNzDyLFjx+KrX/1q/Mu//Ev06NGj3bZLliyJqVOnxsKFC+Piiy+Oe+65Jy677LJ48MEHz/SYAYByDyPz5s2L6dOnx+TJkz+w7ZYtW97XbsqUKXl9W06cOBGNjY0tCgBQms4tdofVq1fHjh078tM0H8bBgwejT58+Leqy7ay+LbW1tfEP/+BeIgBQDooaGWloaIjbbrstfvSjH+WTUT8uNTU1ceTIkeaSPS8AUJqKGhnZvn17HD58OJ/z0eTUqVOxefPmfA5IdnrlnHPOabFP375949ChQy3qsu2svi2VlZV5AQBKX1EjI1/+8pdj9+7dsWvXruZy+eWX55NZs8/fG0Qy48ePj40bN7ao27BhQ14PAFDUyEi3bt1i5MiRLeo+85nPxOc+97nm+tmzZ8eAAQPyeR+Z7LTOxIkTY9GiRfmk12zOybZt2+Khhx46m/0AADqos34F1vr6+jhw4EDz9oQJE2LVqlV5+Bg9enQ88cQTsXbt2veFGgCgPBW9mua9Nm3a1O525vrrr88LAMB7uTcNAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAAHSeMLFu2LEaNGhXdu3fPy/jx4+Opp55qs/3KlSujoqKiRenatevZOG4AoEScW0zjgQMHxn333RcXXXRRFAqF+P73vx8zZ86MnTt3xhe+8IVW98lCy759+5q3s0ACAHBGYWTGjBkttu+99958tGTr1q1thpEsfPTt27eYpwEAysgZzxk5depUrF69Oo4fP56frmnLsWPHYvDgwVFdXZ2Pouzdu/dMnxIAKPeRkczu3bvz8PH222/HBRdcEGvWrIkRI0a02nbYsGHxyCOP5PNMjhw5Et/61rdiwoQJeSDJTvm05cSJE3lp0tjYWOxhAgClOjKSBYxdu3bFCy+8ELfeemvMmTMnXn755VbbZqFl9uzZMWbMmJg4cWI8+eST0atXr1ixYkW7z1FbWxtVVVXNJRtVAQBKU9FhpEuXLjF06NAYO3ZsHhpGjx4dS5Ys+VD7du7cOS699NLYv39/u+1qamrykZSm0tDQUOxhAgDlcp2R06dPtzil8kHzTLLTPP369Wu3XWVlZfPy4aYCAJSmouaMZCMW06ZNi0GDBsXRo0dj1apVsWnTpnjmmWfyx7NTMgMGDMhHTDJ33313XHXVVflIyltvvRX3339/vPrqq3HzzTd/PL0BAEo7jBw+fDgPHAcOHMjncmQTU7Mgcu211+aP19fXR6dOvx9sefPNN2Pu3Llx8ODB6NGjR35qp66urs0JrwBA+SkqjHzve99r9/FslOTdFi9enBcAgLa4Nw0AkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQMcJI8uWLYtRo0ZF9+7d8zJ+/Ph46qmn2t3n8ccfj+HDh0fXrl3jkksuifXr13/UYwYAyjWMDBw4MO67777Yvn17bNu2Lb70pS/FzJkzY+/eva22r6urixtuuCFuuumm2LlzZ8yaNSsve/bsOVvHDwCUUxiZMWNG/Mmf/ElcdNFF8Ud/9Edx7733xgUXXBBbt25ttf2SJUti6tSpsXDhwrj44ovjnnvuicsuuywefPDBs3X8AEC5zhk5depUrF69Oo4fP56frmnNli1bYvLkyS3qpkyZkte358SJE9HY2NiiAAClqegwsnv37nw0pLKyMm655ZZYs2ZNjBgxotW2Bw8ejD59+rSoy7az+vbU1tZGVVVVc6muri72MAGAUg0jw4YNi127dsULL7wQt956a8yZMydefvnls3pQNTU1ceTIkebS0NBwVr8+APDpcW6xO3Tp0iWGDh2afz527Nh46aWX8rkhK1aseF/bvn37xqFDh1rUZdtZfXuyUZesAACl7yNfZ+T06dP5HI/WZHNJNm7c2KJuw4YNbc4xAQDKz7nFnj6ZNm1aDBo0KI4ePRqrVq2KTZs2xTPPPJM/Pnv27BgwYEA+5yNz2223xcSJE2PRokUxffr0fMJrtiT4oYce+nh6AwCUdhg5fPhwHjgOHDiQTyzNLoCWBZFrr702f7y+vj46dfr9YMuECRPywPLNb34zvvGNb+RLgteuXRsjR448+z0BAEo/jHzve99r9/FslOS9rr/++rwAALTGvWkAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBADpOGKmtrY0rrrgiunXrFr17945Zs2bFvn372t1n5cqVUVFR0aJ07dr1ox43AFCOYeT555+PefPmxdatW2PDhg3xzjvvxHXXXRfHjx9vd7/u3bvHgQMHmsurr776UY8bACgR5xbT+Omnn37fqEc2QrJ9+/b44he/2OZ+2WhI3759z/woAYCS9ZHmjBw5ciT/2LNnz3bbHTt2LAYPHhzV1dUxc+bM2Lt3b7vtT5w4EY2NjS0KAFCazjiMnD59OhYsWBBXX311jBw5ss12w4YNi0ceeSTWrVsXP/zhD/P9JkyYEK+99lq7c1OqqqqaSxZiAIDSdMZhJJs7smfPnli9enW77caPHx+zZ8+OMWPGxMSJE+PJJ5+MXr16xYoVK9rcp6amJh91aSoNDQ1nepgAQCnNGWkyf/78+OlPfxqbN2+OgQMHFrVv586d49JLL439+/e32aaysjIvAEDpK2pkpFAo5EFkzZo18dxzz8WQIUOKfsJTp07F7t27o1+/fkXvCwCU+chIdmpm1apV+fyP7FojBw8ezOuzeR3nnXde/nl2SmbAgAH5vI/M3XffHVdddVUMHTo03nrrrbj//vvzpb0333zzx9EfAKCUw8iyZcvyj5MmTWpR/+ijj8bXv/71/PP6+vro1On3Ay5vvvlmzJ07Nw8uPXr0iLFjx0ZdXV2MGDHi7PQAACifMJKdpvkgmzZtarG9ePHivAAAtMa9aQCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAOg4YaS2tjauuOKK6NatW/Tu3TtmzZoV+/bt+8D9Hn/88Rg+fHh07do1Lrnkkli/fv1HOWYAoFzDyPPPPx/z5s2LrVu3xoYNG+Kdd96J6667Lo4fP97mPnV1dXHDDTfETTfdFDt37swDTFb27NlzNo4fAOjgzi2m8dNPP91ie+XKlfkIyfbt2+OLX/xiq/ssWbIkpk6dGgsXLsy377nnnjzIPPjgg7F8+fKPcuwAQLnPGTly5Ej+sWfPnm222bJlS0yePLlF3ZQpU/L6tpw4cSIaGxtbFACgNJ1xGDl9+nQsWLAgrr766hg5cmSb7Q4ePBh9+vRpUZdtZ/XtzU2pqqpqLtXV1Wd6mABAqYaRbO5INu9j9erVZ/eIIqKmpiYfdWkqDQ0NZ/05AIAOOGekyfz58+OnP/1pbN68OQYOHNhu2759+8ahQ4da1GXbWX1bKisr8wIAlL6iRkYKhUIeRNasWRPPPfdcDBky5AP3GT9+fGzcuLFFXTaBNasHADi32FMzq1atinXr1uXXGmma95HN6zjvvPPyz2fPnh0DBgzI531kbrvttpg4cWIsWrQopk+fnp/W2bZtWzz00EMfR38AgFIeGVm2bFk+h2PSpEnRr1+/5vLYY481t6mvr48DBw40b0+YMCEPMFn4GD16dDzxxBOxdu3adie9AgDl49xiT9N8kE2bNr2v7vrrr88LAMB7uTcNAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEDHCiObN2+OGTNmRP/+/aOioiLWrl3bbvtNmzbl7d5bDh48+FGOGwAo1zBy/PjxGD16dCxdurSo/fbt2xcHDhxoLr179y72qQGAEnRusTtMmzYtL8XKwsdnP/vZovcDAErbJzZnZMyYMdGvX7+49tpr4z//8z/bbXvixIlobGxsUQCA0vSxh5EsgCxfvjx+8pOf5KW6ujomTZoUO3bsaHOf2traqKqqai7ZPgBAaSr6NE2xhg0blpcmEyZMiF/96lexePHi+MEPftDqPjU1NXH77bc3b2cjIwIJAJSmjz2MtGbcuHHx85//vM3HKysr8wIAlL4k1xnZtWtXfvoGAKDokZFjx47F/v37m7dfeeWVPFz07NkzBg0alJ9ief311+Nf//Vf88cfeOCBGDJkSHzhC1+It99+Ox5++OF47rnn4tlnnz27PQEAyiOMbNu2La655prm7aa5HXPmzImVK1fm1xCpr69vfvzkyZPxN3/zN3lAOf/882PUqFHx7//+7y2+BgBQvooOI9lKmEKh0ObjWSB5tzvuuCMvAACtcW8aACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAOlYY2bx5c8yYMSP69+8fFRUVsXbt2g/cZ9OmTXHZZZdFZWVlDB06NFauXHmmxwsAlHsYOX78eIwePTqWLl36odq/8sorMX369Ljmmmti165dsWDBgrj55pvjmWeeOZPjBQBKzLnF7jBt2rS8fFjLly+PIUOGxKJFi/Ltiy++OH7+85/H4sWLY8qUKcU+PQBQYj72OSNbtmyJyZMnt6jLQkhW35YTJ05EY2NjiwIAlKaPPYwcPHgw+vTp06Iu284Cxu9+97tW96mtrY2qqqrmUl1d/XEfJgCQyKdyNU1NTU0cOXKkuTQ0NKQ+JADg0zJnpFh9+/aNQ4cOtajLtrt37x7nnXdeq/tkq26yAgCUvo99ZGT8+PGxcePGFnUbNmzI6wEAig4jx44dy5foZqVp6W72eX19ffMpltmzZze3v+WWW+LXv/513HHHHfHLX/4y/vmf/zl+/OMfx1//9V+fzX4AAOUSRrZt2xaXXnppXjK33357/vmdd96Zbx84cKA5mGSyZb0/+9nP8tGQ7Pok2RLfhx9+2LJeAODM5oxMmjQpCoVCm4+3dnXVbJ+dO3cW+1QAQBn4VK6mAQDKhzACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAABJCSMAQFLCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAxwsjS5cujQsvvDC6du0aV155Zbz44otttl25cmVUVFS0KNl+AABnFEYee+yxuP322+Ouu+6KHTt2xOjRo2PKlClx+PDhNvfp3r17HDhwoLm8+uqrvvsAwJmFkW9/+9sxd+7cuPHGG2PEiBGxfPnyOP/88+ORRx5pc59sNKRv377NpU+fPsU+LQBQoooKIydPnozt27fH5MmTf/8FOnXKt7ds2dLmfseOHYvBgwdHdXV1zJw5M/bu3dvu85w4cSIaGxtbFACgNBUVRt544404derU+0Y2su2DBw+2us+wYcPyUZN169bFD3/4wzh9+nRMmDAhXnvttTafp7a2NqqqqppLFmIAgNL0sa+mGT9+fMyePTvGjBkTEydOjCeffDJ69eoVK1asaHOfmpqaOHLkSHNpaGj4uA8TAEjk3GIaf/7zn49zzjknDh061KI+287mgnwYnTt3jksvvTT279/fZpvKysq8AAClr6iRkS5dusTYsWNj48aNzXXZaZdsOxsB+TCy0zy7d++Ofv36FX+0AEB5j4xksmW9c+bMicsvvzzGjRsXDzzwQBw/fjxfXZPJTskMGDAgn/eRufvuu+Oqq66KoUOHxltvvRX3339/vrT35ptvPvu9AQBKP4x85Stfid/+9rdx55135pNWs7kgTz/9dPOk1vr6+nyFTZM333wzXwqcte3Ro0c+slJXV5cvCwYAKDqMZObPn5+X1mzatKnF9uLFi/MCANAa96YBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQAgKWEEAEhKGAEAkhJGAICkhBEAIClhBABIShgBAJISRgCApIQRACApYQQASEoYAQCSEkYAgKSEEQCg44WRpUuXxoUXXhhdu3aNK6+8Ml588cV22z/++OMxfPjwvP0ll1wS69evP9PjBQDKPYw89thjcfvtt8ddd90VO3bsiNGjR8eUKVPi8OHDrbavq6uLG264IW666abYuXNnzJo1Ky979uw5G8cPAJRbGPn2t78dc+fOjRtvvDFGjBgRy5cvj/PPPz8eeeSRVtsvWbIkpk6dGgsXLoyLL7447rnnnrjsssviwQcfPBvHDwB0cOcW0/jkyZOxffv2qKmpaa7r1KlTTJ48ObZs2dLqPll9NpLybtlIytq1a9t8nhMnTuSlyZEjR/KPjY2NxRwuJeToyZMf6fX/v7ff9v4pQ8eOffD75uTvjsU5xz/c++vksaMd7n30f7871uoxv/t7c/R3x6JLB+sXHUPTe6xQKLTfsFCE119/Pftqhbq6uhb1CxcuLIwbN67VfTp37lxYtWpVi7qlS5cWevfu3ebz3HXXXfnzKIqiKIoSHb40NDS0my+KGhn5pGQjL+8eTTl9+nT87//+b3zuc5+LioqKM05n1dXV0dDQEN27d49you/l1/dy7XdG38uv7+Xa747Q92xE5OjRo9G/f/922xUVRj7/+c/HOeecE4cOHWpRn2337du31X2y+mLaZyorK/Pybp/97GfjbMherE/jC/ZJ0Pfy63u59juj7+XX93Lt96e971VVVWd3AmuXLl1i7NixsXHjxhajFtn2+PHjW90nq393+8yGDRvabA8AlJeiT9Nkp0/mzJkTl19+eYwbNy4eeOCBOH78eL66JjN79uwYMGBA1NbW5tu33XZbTJw4MRYtWhTTp0+P1atXx7Zt2+Khhx46+70BAEo/jHzlK1+J3/72t3HnnXfGwYMHY8yYMfH0009Hnz598sfr6+vzFTZNJkyYEKtWrYpvfvOb8Y1vfCMuuuiifCXNyJEj45OUnfbJro3y3tM/5UDfy6/v5drvjL6XX9/Ltd+l1PeKbBZr6oMAAMqXe9MAAEkJIwBAUsIIAJCUMAIAJFX2YSS7B062Iii7suuuXbui1P3pn/5pDBo0KLp27Rr9+vWLr33ta/Gb3/wmSt1///d/53eOHjJkSJx33nnxh3/4h/kM9Ox+S+Xg3nvvzVe2ZTe1PFsXEPw0Wrp0aVx44YX5+/vKK6+MF198McrB5s2bY8aMGflVLrPfZe3d+6uUZJeQuOKKK6Jbt27Ru3fv/I7w+/bti3KwbNmyGDVqVPPFzrJrdz311FPRUZV9GLnjjjs+8DK1peSaa66JH//4x/kP7E9+8pP41a9+FX/2Z38Wpe6Xv/xlfoG+FStWxN69e2Px4sX5Haez5eblIAtd119/fdx6661Rqh577LH8OkhZyNyxY0eMHj06vynn4cOHo9Rl13rK+puFsXLy/PPPx7x582Lr1q35xTTfeeeduO666/LvR6kbOHBg3HffffnNa7Nrd33pS1+KmTNn5r/fOqRCGVu/fn1h+PDhhb179+Y38tm5c2eh3Kxbt65QUVFROHnyZKHc/NM//VNhyJAhhXLy6KOPFqqqqgqlKLtZ57x585q3T506Vejfv3+htra2UE6y32Vr1qwplKPDhw/n/X/++ecL5ahHjx6Fhx9+uNARle3ISHZ/nLlz58YPfvCDfOi6HGU3H/zRj36UD9937tw5ys2RI0eiZ8+eqQ+DszTyk/2HOHny5Oa67OKL2faWLVuSHhuf7M90ptx+rk+dOpVf3TwbEeqot1opyzCS/fPw9a9/PW655Zb8svbl5m//9m/jM5/5TH4X5OyKuevWrYtys3///vjud78bf/mXf5n6UDgL3njjjfwXctOVoJtk29mVoil92WnYBQsWxNVXX/2JX+E7ld27d8cFF1yQX301+3u2Zs2aGDFiRHREJRVG/u7v/i6fvNVeyeYOZH+Eslsa19TURDn1u8nChQtj586d8eyzz+Z3Yc7uJ9RRL8RbbN8zr7/+ekydOjWfQ5GNjnVUZ9J3KFXZ3JE9e/bkIwTlYtiwYfnCixdeeCGfD5bdN+7ll1+OjqikLgef3TPnf/7nf9pt8wd/8Afx53/+5/Fv//Zv+S/rJtl/Vdkf5q9+9avx/e9/P0qx39ldl9/rtddei+rq6qirq+uQw3vF9j1bOTRp0qS46qqrYuXKlS3uo9TRnMnrnvU5++/xrbfeilI7TZOdbn3iiSfyFRVNsl/OWV/LafQv+72W/Yf87u9DqZs/f37+GmerirIVc+Vq8uTJ+UrBbKJ+yd8o79OsV69eefkg3/nOd+If//Efm7ezP1DZrPtsNn62HLBU+93W0GbTEueOqJi+ZyMi2WqisWPHxqOPPtqhg8hHfd1LTRa4std148aNzX+Es/d2tp39oaI0Zf9L/9Vf/VUevjZt2lTWQaTpPd9Rf5eXVBj5sLLrbLxbds4tkyXKbLlUqcqG8l566aX44z/+4+jRo0e+rPfv//7v8353xFGRYmRBJBsRGTx4cHzrW9/KRxWa9O3bN0pdNjcom7CcfcxGAZuuqTN06NDm939Hly3rzUZCsnlg48aNiwceeCCf0HfjjTdGqTt27Fg+D6rJK6+8kr/G2UTO9/6+K7VTM9ld4bNRkexaI03zg6qqqvLrCZWympqamDZtWv76ZtMOsu9DFsieeeaZ6JBSL+f5NHjllVfKYmnvL37xi8I111xT6NmzZ6GysrJw4YUXFm655ZbCa6+9ViiHJa3Za9xaKQdz5sxpte//8R//USgl3/3udwuDBg0qdOnSJV/qu3Xr1kI5yF7H1l7f7HUvZW39TGc/76XuL/7iLwqDBw/O3+u9evUqfPnLXy48++yzhY6qpOaMAAAdT8c+aQ4AdHjCCACQlDACACQljAAASQkjAEBSwggAkJQwAgAkJYwAAEkJIwBAUsIIAJCUMAIAJCWMAACR0v8HsWaiBvfxbRAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(action_list, bins=30, density=True, alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f3861-a1ed-466b-87b9-dd65c1213ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a1ae2-c9a8-4189-bf85-69e332129075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b136c6a-7f35-48b5-a6b2-6d4eb8679ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49addc-91c4-499f-80d4-a2d1f2d6bf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
