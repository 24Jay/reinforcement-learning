{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7598da2c-2e72-4d40-8e69-85c0bc3129ec",
   "metadata": {},
   "source": [
    "## PPO: Proximal Policy Optimization Algorithms\n",
    "\n",
    "[Paper: Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347)\n",
    "\n",
    "[Youtube: Natural Policy Gradients, TRPO, PPO](https://www.youtube.com/watch?v=xvRrgxcpaHY)\n",
    "\n",
    "https://www.youtube.com/watch?v=k2pD3k1485A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1029ee-64d3-42ce-b32c-1def928a2535",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "PPO：\n",
    "- surrogate object function\n",
    "- 一个样本经历多个epoch minibatch updates，而常规的Policy Gradient算法perform one gradient update per data sample\n",
    "- 具备TRPO的优点但是更简单、better sample complexity\n",
    "\n",
    "Q-learning:\n",
    "- fails on many simple problems and is poorly understood\n",
    "\n",
    "Vanilla PG:\n",
    "- have poor data effiency and robustness\n",
    "\n",
    "TRPO:\n",
    "- complicated and is not compitable with architectures that include noise or parameter sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7584f-350a-4f19-879c-29263f3659bc",
   "metadata": {},
   "source": [
    "### 2.1 Policy Gradient Methods\n",
    "目标奖励函数：\n",
    "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}[log\\pi_{\\theta}(a_t|s_t)\\hat{A}_t]$$\n",
    "\n",
    "其中$\\hat{A}_t$ is an estimator of the advantage function at timestep t.\n",
    "$$\\hat{A}_t = q(a_t, s_t) - b(s_t)$$\n",
    "\n",
    "使用同样的轨迹进行多次策略优化很有吸引力，但是一直未得到理论证明，并且可能会导致破坏性的策略更新：\n",
    "> While it is appealing to perform multiple steps of optimization on this loss $L^{PG}$ using the same trajectory, doing so is not well-justified, and empirically it often leads to destructively large policy updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06797a88-bb37-41dd-9138-d1894e0b6f84",
   "metadata": {},
   "source": [
    "## 2.2 TRPO\n",
    "目标奖励函数：\n",
    "$$\\underset{\\theta}{maxmize}\\quad\\hat{\\mathbb{E}}[\\cfrac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t]$$\n",
    "$$s.t.\\quad \\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(*|s_t), \\pi_{\\theta_{old}}(*|s_t)]]\\le \\delta$$\n",
    "因此TRPO本质上求解的是以下最优化问题：\n",
    "$$\\underset{\\theta}{maxmize} \\mathbb{\\hat{E}}[\\cfrac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{old}(a_t|s_t)}\\hat{A}_t - \\beta KL[\\pi_{old}(a_t|s_t), \\pi_{\\theta}(a_t|s_t)]]$$\n",
    "\n",
    "TRPO使用硬约束而非将约束带入优化目标作为惩罚项，是因为难以找到单一固定的惩罚系数$\\beta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbd5c8ca-4d7b-4137-872f-5860f632379c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. PPO 1 : Clipped Surrogate Objective\n",
    "重要性采样系数 : $r_t(\\theta) = \\cfrac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$, so $r_{old}({\\theta})=1$。\n",
    "#### TRPO最大化代理目标函数：(CPI: conservative policy iteration)\n",
    "$$L^{CPI}(\\theta) = \\hat{\\mathbb{E}}[\\cfrac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t] = \\hat{\\mathbb{E}}[r_t(\\theta)\\hat{A}_t]$$\n",
    "直接优化上述目标会导致很大幅度的策略更新，因此对于策略更新的幅度（$r_t(\\theta)$偏离1的程度）进行惩罚。\n",
    "\n",
    "#### PPO最大化下面代理目标函数：\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}[min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A_t}]$$\n",
    "\n",
    "- 第一项$r_t(\\theta)\\hat{A}_t$就是$L^{CPI}$\n",
    "- 第二项$clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A_t}$截断目标，removes the incentive for moving $r_t$ outside of the interval $[1-\\epsilon, 1+\\epsilon]$\n",
    "- min(第一项，第二项)：最终的目标取第一项和第二项的最小值，so the final objective is a lower bound(pessimistic bound) on the unclipped objective\n",
    "\n",
    "如果没有约束条件，最大化$L^{CPI}$将会导致excessively large policy update, 因此需要考虑如何**修改上述目标函数以惩罚policy changes that move $r_t(\\theta)$ away from 1**\n",
    "\n",
    "* $L^{CLIP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248fdb6-c912-4635-b82a-0daaba5927a9",
   "metadata": {},
   "source": [
    "|$A_t$|$0\\lt r_t\\lt \\infty$|$clip$|$min(r_tA_t, clip)$|\n",
    "|--|------|----|------|\n",
    "|+|$1+\\epsilon \\lt r_t\\lt +\\infty$|$1+\\epsilon$|$(1+\\epsilon)*A_t$|\n",
    "|+|$1-\\epsilon \\lt r_t \\lt 1+\\epsilon$|$r_t$|$r_t*A_t$|\n",
    "|+|$0\\lt r_t \\lt 1-\\epsilon$|$1-\\epsilon$|$r_t*A_t$|\n",
    "|-|$1+\\epsilon \\lt r_t \\lt +\\infty$|$1+\\epsilon$|$r_t*A_t$|\n",
    "|-|$1-\\epsilon \\lt r_t \\lt 1+\\epsilon$|$r_t$|$r_t*A_t$|\n",
    "|-|$0 \\lt r_t \\lt1-\\epsilon$|$1-\\epsilon$|$(1-\\epsilon)*A_t$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200f520-c55d-48e0-b26f-7e081cefede8",
   "metadata": {},
   "source": [
    "#### 策略更新对比\n",
    "* 代理目标奖励函数\n",
    "在 PPO（近端策略优化）论文的 Figure 2 中，** 线性插值因子（linear interpolation factor）** 是理解代理目标函数$L^{\\text{CLIP}}$与原始目标$L^{\\text{CPI}}$关系的关键。\\\n",
    "Figure 2 的实验基于连续控制问题，通过策略更新方向的线性插值（即从旧策略$\\theta_{\\text{old}}$到新策略$\\theta_{\\text{new}}$的插值），展示了多个目标函数的变化趋势。具体机制如下：\n",
    "- 插值公式：\n",
    "    - 设插值因子为$\\epsilon \\in [0, 1]$，中间策略为$\\theta_{\\epsilon} = \\theta_{\\text{old}} + \\epsilon (\\theta_{\\text{new}} - \\theta_{\\text{old}})$。\n",
    "    - 当$\\epsilon = 0$时，策略为旧策略；\n",
    "    - 当$\\epsilon = 1$时，策略为新策略。\n",
    "    - 中间值表示不同幅度的策略更新。\n",
    "\n",
    "- 目标函数对比：\n",
    "    - 实验对比了$L^{\\text{CLIP}}$（裁剪后的代理目标）与$L^{\\text{CPI}}$（保守策略迭代的原始目标），以及可能的其他目标（如 KL 散度、奖励函数等）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae2913f-8654-4700-ad1d-a0816769c9f4",
   "metadata": {},
   "source": [
    "## 4. PPO 2: Adaptive KL Penalty Coefficient $\\beta$\n",
    "将KL散度约束作为惩罚项放到目标函数，同时自适应调节KL项惩罚系数$\\beta$\n",
    "$$L^{KLPEN}(\\theta) = \\hat{\\mathbb{E}}[\\cfrac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t] - \\beta KL[\\pi_{old}(\\cdot|s_t), \\pi_{\\theta}(\\cdot|s_t))]$$\n",
    "\n",
    "根据KL散度取值$d = \\mathbb{\\hat{E}}_t[KL[\\pi_{old}(\\cdot|s_t), \\pi_{\\theta}(\\cdot|s_t))]]$自适应调节$\\beta$:\n",
    "- if $d < d_{tar}/1.5, \\beta \\leftarrow \\beta/2$\n",
    "- if $d > d_{tar} \\times 1.5, \\beta \\leftarrow \\beta\\times2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e3e6729-2e33-48b5-99d7-ea7d74f4432f",
   "metadata": {},
   "source": [
    "## 5. Algorithm\n",
    "1. 很多算法为了降低优势函数的估计误差都会学习状态值函数$v(s)$，因此可以使用actor和critic共享的架构，例如:\n",
    "    - [High-Dimensional Continuous Control Using Generalized Advantage Estimation\n",
    "](https://arxiv.org/abs/1506.02438)\n",
    "    - [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)\n",
    "2. 为了保证算法探索性能可以增加entropy bonus\n",
    "\n",
    "$$L_t^{CLIP + VF + S}(\\theta) = \\mathbb{\\hat{E}}_t[L_t^{CLIP}(\\theta)- c_1L_t^{VF}(\\theta) + c_2S[\\pi_{\\theta}](s_t)]$$\n",
    "其中$L_t^{VF}(\\theta) = (V_{\\theta}(s_t) - V_t^{target})^2$\n",
    "\n",
    "3. 有些PG算法runs the policy for $T$ timesteps，使用以下优势函数：\n",
    "$$\\hat{A}_t = r_t + \\gamma r_{t+1} + ... + \\gamma ^{T-t+1}r_{T-1} + \\gamma ^{T-t}V(s_T) - V(s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f004f4-4615-4f9a-8851-d1b54cdfe084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
